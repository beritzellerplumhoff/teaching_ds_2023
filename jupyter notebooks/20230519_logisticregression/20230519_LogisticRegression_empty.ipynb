{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6745247b",
   "metadata": {},
   "source": [
    "## 19.05.2023 Logistic Regression\n",
    "\n",
    "Copyright (C) 2023, B. Zeller-Plumhoff\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389557e3",
   "metadata": {},
   "source": [
    "This Jupyter Notebook was created by Berit Zeller-Plumhoff for the course \"Data Science for Material Scientists\" at Kiel University. \n",
    "\n",
    "Within the notebook, you will plot a sigmoid function trialling different parameters. You will then use logistic regression to attempt a simplistic prediction of tumour malignance based on one morphological feature of the tumour. Finally, you will perform a logistic regression to classify metallic and non-metallic materials based on their composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550ddff",
   "metadata": {},
   "source": [
    "We begin by loading the required libraries. Note that in addition to the libraries you have gotten to know until now, we make use of [matminer](https://hackingmaterials.lbl.gov/matminer/#) and [pymatgen](https://pymatgen.org/). Have a closer look at the respective websites to learn more about these. \n",
    "\n",
    "The publication from [Ward et al., 2018](https://www.sciencedirect.com/science/article/abs/pii/S0927025618303252)<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) gives further information on Matminer. The toolkit accesses external databases, such as the [Materials Project](https://materialsproject.org/), where a large number of materials data has been accumulated and published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a437697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # library for organizing data\n",
    "import numpy as np # library for numerial computations\n",
    "from sklearn import linear_model # the linear_model library establishes a straightforward implementation of a linear regression model\n",
    "from sklearn.metrics import log_loss, accuracy_score # these libraries enable the calculation of the MSE, MAE and R2 goodness of fit\n",
    "from sklearn.model_selection import train_test_split # this library enables the splitting of a data set into training and test data\n",
    "from sklearn.datasets import load_breast_cancer # access a medical dataset of breast cancer malignancy given morphological features\n",
    "from sklearn.inspection import DecisionBoundaryDisplay # library to display decision boundaries of classifiers\n",
    "\n",
    "import matplotlib.pyplot as plt # library for plotting (not interactive)\n",
    "import matminer.datasets as mm # library for data mining materials properties, accesses published datasets\n",
    "from matminer.featurizers.conversions import StrToComposition # converts a string denoting a material composition into the composition\n",
    "from matminer.featurizers.composition.element import ElementFraction # determines the element fraction for a given composition\n",
    "from pymatgen.core import Composition # materials analysis library, module used to analyse the chemical composition of a compound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a895c",
   "metadata": {},
   "source": [
    "#### Logistic regression function\n",
    "\n",
    "We will begin by gaining some practical understanding on the sigmoid function used in logistic regression. To start with, define a function that takes both the (1D) feature vector $x$ and the array $\\theta$ (containing the intercept $\\theta_0$ and slope $\\theta_1$) as input and returns the values of the sigmoid function for all given $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3073d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_func():\n",
    "    \n",
    "    # determine the value of the sigmoid function\n",
    "    \n",
    "    \n",
    "    #return the result\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1275e4",
   "metadata": {},
   "source": [
    "Based on the function you have defined, make a figure with two subplots (arranged vertically), where in the first subplot, you vary $\\theta_0 \\in \\{-4,0,4\\}$, while $\\theta_1=1$ and in the second subplot you vary $\\theta_1 \\in \\{-4,0.5,1,4\\}$, while $\\theta_0=0$. $x \\in \\left[-10,10\\right]$ with $\\Delta x = 0.1$. Make sure that you are including a legend in each plot. Let both plots share the x-axis, so that tick and the axis label only need to be included in the lower plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307da5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set x\n",
    "x=\n",
    "# initialize the subplots\n",
    "\n",
    "# adjust the vertical spacing between the plots to minimize it\n",
    "\n",
    "# plot the function for varying theta_0 in the upper graph\n",
    "for i in \n",
    "\n",
    "# plot the function for varying theta_1 in the lower graph\n",
    "for i in \n",
    "\n",
    "# show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8af27",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "While we have an understanding of what how $\\theta$ will influence the shape of the logistic regression function, we now what to actually employ it for classification. Define a classification function that makes use of _scikit_learn_ to perform classification using logistic regression given an input feature vector and observation vector for training. The function should output the predicted labels/classes for the training features, as well as the fitted classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35013da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification():\n",
    "    \n",
    "    # Define the model using linear_model and LogisticRegression from Scikit_learn\n",
    "    model = \n",
    "    \n",
    "    # train the model using .fit\n",
    "    \n",
    "        \n",
    "    # Use the model to predict the entire set of data using .predict\n",
    "    \n",
    "    \n",
    "    # return the predictions and the fitted model\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfff5f5",
   "metadata": {},
   "source": [
    "We want to apply this classier to the [Breast Cancer Wisconsin Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)), which is available in _scikit_learn_. Start by loading and displaying the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the breast cancer dataset into a variable\n",
    "\n",
    "# display that variable to attain a better understanding of its structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afe369",
   "metadata": {},
   "source": [
    "Without specifing any parameters when loading the dataset, you have loaded a bunch object. You should convert it into a pandas data frame. Display the data frame to have a look at the different features. Note that a target of 1 means that the tumour is benign, while a target of 0 means that it is malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data part of the cancer bunch object to a dataframe\n",
    "\n",
    "# add a target column to the dataframe to which you assign the target of the cancer bunch object\n",
    "\n",
    "# display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8c8a0",
   "metadata": {},
   "source": [
    "As you can see, within this data set we have 30 features that can be used to predict the cancer malignancy. In this instance, we will not use all features simultaneously, but you should test for each feature whether it alone could be used to predict the tumour malignancy with little error. Save all trained models in a list for later access. Display the log loss testing error for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08876c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the target column of the data frame to your observation variable y\n",
    "\n",
    "# initialize empty lists for the log loss and the different models for each feeature\n",
    "\n",
    "# perform training the classifier for each feature\n",
    "for i in \n",
    "    # assign the feature vector for training\n",
    "    \n",
    "    # split the data into training and test data\n",
    "    \n",
    "    # perform regression and prediction for training and predict y for the testing data\n",
    "    \n",
    "    \n",
    "    # add the model and error to the respective lists\n",
    "    \n",
    "    \n",
    "\n",
    "# plot the error for each feature in a horizontal bar chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524f44b",
   "metadata": {},
   "source": [
    "You can see how the use of different features for the classification of the cancer might lead to very different results. Perform a classification with the best performing feature and plot the classification probability for the features (selecting only every 5th data point), highlighting the observed class by color of the marker. In addition to the scatter plot, plot the underlying probability function and indicate the probability treshold by a horizontal line. Include a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b065897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the column index for which the log loss error is minimal\n",
    "min_idx=\n",
    "# initialize the figure\n",
    "\n",
    "# plot every 5th entry of the feature vector and the predicted probability determined by the classifer\n",
    "# each scatter marker should be assined the correct class in colour\n",
    "\n",
    "# set up an array in the overall range of the feature vector with minimal log loss error and even step size\n",
    "\n",
    "# plot the underlying probability function with the vector you have just defined\n",
    "\n",
    "# plot the horizontal threshold line\n",
    "\n",
    "# prepare your legend\n",
    "\n",
    "# add axes labels and limits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5f4d5",
   "metadata": {},
   "source": [
    "Assuming that we may not want to use one but two features for the classification, perform the classification with the __worst area__ and __worst perimeter__. Use all the datasets of these two features for training. We now want to plot the 2D decision boundary for the resulting classifier. Follow the first example given [here](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html) to plot the decision boundary underneath a 2D scatterplot of the features and their designated target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your feature and observation vector for training\n",
    "\n",
    "\n",
    "# perform the classification based on the vectors you defined\n",
    "\n",
    "\n",
    "# generate two 2D features using numpy meshgrid\n",
    "\n",
    "\n",
    "# create a grid based on the 2D features (see the online example)\n",
    "\n",
    "\n",
    "# generate the prediction for the grid and reshape it into 2D \n",
    "\n",
    "\n",
    "# created the decision boundary\n",
    "\n",
    "\n",
    "# set the colour map for your plot and display both the decision boundary, as well as the scatter plot of the\n",
    "# actual features and their actual classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205fc2b2",
   "metadata": {},
   "source": [
    "What do you observe? Comment on the quality of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc820e82",
   "metadata": {},
   "source": [
    "### Predicting metallicity\n",
    "\n",
    "Finally, we want to apply logistic regression to a materials science dataset. Therefore, we will access the dataset based on the publication from [Zhuo et al., 2018](https://pubs.acs.org/doi/pdf/10.1021/acs.jpclett.8b00124)<a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2) that can be accessed through __matminer__. Use the _load_dataset_ function to load the __matbench_expt_is_metal__ dataset - it will automatically be loaded as a pandas dataframe. Display the dataset.\n",
    "\n",
    "You can get a more detailed step by step introduction of different features that we will explore in this section by looking at [this lesson](https://workshop.materialsproject.org/lessons/08_ml_matminer/matminer-notes/) of the materials project workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411e567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72187fc5",
   "metadata": {},
   "source": [
    "As you can see, the dataset contains the composition of each compoung and whether or not this is a metal. The latter information was determined based on the experimentally determined bandgap. You can get more information on the dataset by _printing_ the return of the function _get_all_dataset_info_, which takes the name of the dataset as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d1374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22d47f84",
   "metadata": {},
   "source": [
    "As you can see, the composition given in the _composition_ column of the dataframe is a string. To use the composition as features, we ultimately want to create a number of feature vectors equal to the number of elements in the periodic table where for any observation of a certain compound, the entry to that feature vector contains the stochiometric fraction of said element. To visualize this, run the following cell. Comment on what operations have been performed here. Play around with the dataframe entry that is evaluated to see how the output changes. Discuss what you are seeing with the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is partly taken and adapted from https://workshop.materialsproject.org/lessons/08_ml_matminer/matminer-notes/\n",
    "# which is available under the BSD 3-Clause License, Copyright (c) 2019, shreddd, All rights reserved.\n",
    "# For the list of conditions of the license, please see https://choosealicense.com/licenses/bsd-3-clause/\n",
    "\n",
    "idx=4\n",
    "form=df[\"composition\"][idx]\n",
    "print(form)\n",
    "comp=Composition(df[\"composition\"][idx])\n",
    "print(comp)\n",
    "ef=ElementFraction()\n",
    "element_fraction_labels = ef.feature_labels()\n",
    "print(element_fraction_labels)\n",
    "compef=ef.featurize(comp)\n",
    "print(compef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ca2c3",
   "metadata": {},
   "source": [
    "We now want to apply this operation to the whole dataset. To this end, we wish to use the _featurize_dataframe_ operation from _ElementFraction()_ but need to process our dataframe first to be able to do so.\n",
    "\n",
    "In the first step, rename the column __composition__ into __formula__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a727987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeda22ef",
   "metadata": {},
   "source": [
    "Secondly, we need to convert the chemical formula to a composition, which can be done by using _StrToComposition_ and its _featurize_dataframe_ operation. Apply this to the dataframe and display it. This operation may take around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stc = \n",
    "df = \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c2145",
   "metadata": {},
   "source": [
    "You can now apply the _featurize_dataframe_ operation from _ElementFraction()_ to the dataframe. However, this will take approximately 10 minutes. If you want to take a break, uncomment and run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa634de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = ef.featurize_dataframe(df, \"composition\", ignore_errors=True)\n",
    "#df.to_pickle('./ismetal_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368751b",
   "metadata": {},
   "source": [
    "Otherwise, I have saved the final dataframe as .pkl for you to load at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1efda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df=pd.read_pickle('ismetal_df.pkl')\n",
    "load_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb7a0f",
   "metadata": {},
   "source": [
    "Assign the loaded dataframe to your original one and look at some scatter plots for some elements (displaying the element column in $x$ and the target value in $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc066e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46a1eb46",
   "metadata": {},
   "source": [
    "Finally, we are ready to perform our classification task. Assign the dataframe __without__ columns formula\" \"is_metal\" and \"composition\" to your feature variable X and the \"is_metal\" column to the observation y. Then split this data into training and test data and train the classifier. Predict the classes for the testing data and calculate and display both the log loss error for training and testing data, as well as the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d938dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the variables\n",
    "\n",
    "# split your data sets\n",
    "\n",
    "\n",
    "# perform classification and prediction for training and testing data\n",
    "\n",
    "\n",
    "# print log loss and accuracy for training and testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f65cd1",
   "metadata": {},
   "source": [
    "What do these results mean? Discuss.\n",
    "\n",
    "If you have time left, run the following cell and look up the function that are used in the _scikit_learn_ documentation. What operations are carried out and what does the final result mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42098a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits=30, random_state=1, shuffle=True)\n",
    "\n",
    "lr = linear_model.LogisticRegression()\n",
    "scores = cross_val_score(lr, X, y, scoring='accuracy', cv=kfold)\n",
    "\n",
    "print('Mean accuracy: {:.3f}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be8064",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1.[^](#cite_ref-1) Ward, L., Dunn, A., Faghaninia, A., Zimmermann, N. E. R., Bajaj, S., Wang, Q.,\n",
    "Montoya, J. H., Chen, J., Bystrom, K., Dylla, M., Chard, K., Asta, M., Persson, K., Snyder, G. J., Foster, I., Jain, A., Matminer: An open source toolkit for materials data mining. Comput. Mater. Sci. 152, 60-69 (2018).\n",
    "\n",
    "<a name=\"cite_note-2\"></a>2.[^](#cite_ref-2) Y. Zhuo, A.M. Tehrani, and J. Brgoch, J. Phys. Chem. Lett. 2018, 9, 7, 1668–1673, https://doi.org/10.1021/acs.jpclett.8b00124"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
