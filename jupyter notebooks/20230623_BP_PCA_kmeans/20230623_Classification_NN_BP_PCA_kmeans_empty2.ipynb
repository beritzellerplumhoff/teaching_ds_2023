{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389557e3",
   "metadata": {},
   "source": [
    "## 23.06.2023 Backpropagation in neural networks, PCA and k-means clustering\n",
    "\n",
    "Copyright (C) 2023, B. Zeller-Plumhoff\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.html) for more details.\n",
    "\n",
    "This Jupyter Notebook was created by Berit Zeller-Plumhoff for the course \"Data Science for Material Scientists\" at Kiel University. \n",
    "\n",
    "Within the notebook, you will see how the backpropagation is performed for a neural network and how the weights are being updated. Following that, we will implement the PCA and k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550ddff",
   "metadata": {},
   "source": [
    "We begin by loading the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a437697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # library for organizing data\n",
    "import numpy as np # library for numerial computations\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score # this library enables the splitting of a data set into training and test data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from numpy.linalg import eigh\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import matplotlib.pyplot as plt # library for plotting (not interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc820e82",
   "metadata": {},
   "source": [
    "### Predicting metallicity\n",
    "\n",
    "For classification of metallicit based on the material compositon, we will use the dataset based on the publication from [Zhuo et al., 2018](https://pubs.acs.org/doi/pdf/10.1021/acs.jpclett.8b00124)<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) that can be accessed through __matminer__. Have a look at last week's notebook to learn more about the dataset and __matminer__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368751b",
   "metadata": {},
   "source": [
    "In this case, we will start by loading the pickle file which we prepared previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1efda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle('ismetal_df.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1eb46",
   "metadata": {},
   "source": [
    "Assign the dataframe __without__ columns \"formula\" \"is_metal\" and \"composition\" to your feature variable X and the \"is_metal\" column to the observation y. Then split this data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1592db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the variables\n",
    "X=df.drop([\"formula\",\"is_metal\",\"composition\"],axis=1)\n",
    "y=df[\"is_metal\"]\n",
    "\n",
    "# split your data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39a987",
   "metadata": {},
   "source": [
    "With trees and random forests we can determine the [__feature_importances__](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) in a straightforward manner. The following cell lets you compute the feature importances based on the mean decrease in Gini impurity for the random forests you have trained and plot them in descending order. How does this agree with the tree you plotted in the previous cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf69b1e",
   "metadata": {},
   "source": [
    "#### Neural network architecture\n",
    "\n",
    "As a next step we want to better understand the architecture of the best neural network that we have created. Lets assess the weights of our 4 most relevant features until now, which appeared to be the content of Se, S, O and Te, as well as a less important feature, such as H. Using the __get_loc__ functionality, find the column index they correspond to. Similarly, find the configuration where the cross validation score is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols='H','Se','S','O','Te'\n",
    "for c in cols:\n",
    "    print(X.columns.get_loc(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12942d",
   "metadata": {},
   "source": [
    "How does the network architecture look, if you retrain the best classifier based on our initial train/test split? Can you draw a graph (including only these 5 input features)? To do so, use the NN_Vis.py file. The code in this file was taken from the entries of Milo and Oliver Wilken [here](https://stackoverflow.com/questions/29888233/how-to-visualize-a-neural-network/37366154#37366154), and is available under a [CC BY-SA 3.0 license](https://creativecommons.org/licenses/by-sa/3.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfda619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_Vis as VisNN\n",
    "clf=MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "network_structure = np.hstack(([5], np.asarray(clf.hidden_layer_sizes), [1]))\n",
    "\n",
    "# Draw the Neural Network with weights\n",
    "network=VisNN.DrawNN(network_structure)\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80132e6a",
   "metadata": {},
   "source": [
    "#### Partial training of network\n",
    "\n",
    "If you are interested in obtaining the test loss during training of your network, you may use the __partial_fit__ functionality of the MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa16a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=np.unique(y)\n",
    "# neural network (loop over number of hidden layers)\n",
    "model_nn=[]\n",
    "y_pred_nn=[]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "for k in range(1,3):\n",
    "    hl=np.ones((k))*10\n",
    "    model_nn_tmp = MLPClassifier(hidden_layer_sizes=(hl.astype(int)), max_iter=500, random_state=42)\n",
    "    lloss=[]\n",
    "    for j in range(1,501):\n",
    "        model_nn_tmp.partial_fit(X_train, y_train,classes=classes)\n",
    "        lloss.append(log_loss(y_test, model_nn_tmp.predict_proba(X_test)))\n",
    "    plt.plot(np.arange(1,501),lloss,label='# hidden layers =%s' %(int(k)))\n",
    "plt.ylabel('Test loss')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1.02))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421dfb1d",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "In this part of the notebook, you will implement a PCA for the metallicity dataset. We begin by performing the standardization of our dataframe. For each feature $x$, compute $$x^{(i)}=\\frac{x^{(i)}-\\bar{x}}{\\sigma(x)}$$ where $\\bar{x}$ is the mean value of $x$ and $\\sigma(x)$ the standard deviation. Replace NaNs with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ceaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad345fb8",
   "metadata": {},
   "source": [
    "The next step in the PCA is to compute the covariance matrix. Use the [__numpy covariance__](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) to do so. Display the covariance matrix, including axes labels and a colour bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9dac86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0adaff1e",
   "metadata": {},
   "source": [
    "In the next step, we perform the singular value decomposition of the covariance matrix, i.e. we will determine $\\lambda$ and $v$ for which $$Cov_X \\cdot v = \\lambda v$$\n",
    "Use the function [__eigh__](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html) from numpy.linalg. Compute the explained variance ratio for the eigenvalues (singular values) and sort these in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39823e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62185493",
   "metadata": {},
   "source": [
    "Plot the explained variance ratio in the cumulative scree plot. Include a vertical line to highlight at which number of principal components 99% of the variance are explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a096bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a6e4295",
   "metadata": {},
   "source": [
    "Thus, instead of using the 103 features for predicting metallicity, we could transform them into their 78 most relevant principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853223f8",
   "metadata": {},
   "source": [
    "### k-means clustering\n",
    "\n",
    "All the work we have performed until now in terms of classification was supervised machine learning, i.e. where we trained our classifiers with ground truth training data to predict unknown test data sets. We can also perform unsupervised machine learning. In this respect, we may use [__k-means__](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering, for example to cluster our metallicity dataset. Output the accuracy of the clusters you would predict. What are you observing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22374958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6d7cf30",
   "metadata": {},
   "source": [
    "In order to observe how k-means clustering and the elbow method works, we will use the iris dataset. Load the dataset and display sepal width vs. sepal length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baece097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data\n",
    "x1 = np.array([3, 1, 1, 2, 1, 6, 6, 6, 5, 6,\\\n",
    "               7, 8, 9, 8, 9, 9, 8, 4, 4, 5, 4])\n",
    "x2 = np.array([5, 4, 5, 6, 5, 8, 6, 7, 6, 7, \\\n",
    "               1, 2, 1, 2, 3, 2, 3, 9, 10, 9, 10])\n",
    "X = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\n",
    " \n",
    "# Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f3460",
   "metadata": {},
   "source": [
    "Perform the clustering for different $k$ and compute the distortion using __cdist__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba723e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f637c5",
   "metadata": {},
   "source": [
    "Plot the distortion for different values of k. Which k would you select?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332039b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e282bd",
   "metadata": {},
   "source": [
    "Let's plot how the __kmeans__ clustering is finding the clusters during fitting for 3 clusters for the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,51):\n",
    "    kmeans = KMeans(n_clusters=3,init='random',n_init=1,max_iter=i,random_state=1)\n",
    "    y_kmeans = kmeans.fit_predict(data.data)\n",
    "    plt.scatter(data.data[:, 0], data.data[:, 1], c=y_kmeans)\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0],\\\n",
    "        kmeans.cluster_centers_[:, 1], \\\n",
    "        s=100, c='red')\n",
    "    plt.title('K-means clustering (k=3)')\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.savefig('kmeans/kmeans_%s.png' % (i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f009ea1",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1.[^](#cite_ref-1) Y. Zhuo, A.M. Tehrani, and J. Brgoch, J. Phys. Chem. Lett. 2018, 9, 7, 1668–1673, https://doi.org/10.1021/acs.jpclett.8b00124"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
