{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389557e3",
   "metadata": {},
   "source": [
    "## 09.06.2023 ROC, k-nearest neighbour, decision trees and random forests\n",
    "\n",
    "Copyright (C) 2023, B. Zeller-Plumhoff\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.html) for more details.\n",
    "\n",
    "This Jupyter Notebook was created by Berit Zeller-Plumhoff for the course \"Data Science for Material Scientists\" at Kiel University. \n",
    "\n",
    "Within the notebook, you will perform a classification of metallicity using logistic regression, $k$-nearest neighbours and decision trees. Varying the $k$ and depth of the tree, you will assess how these influence your results and compare the performance of the different classifiers using ROC metrics and curves.\n",
    "\n",
    "<span style='color:red'> 02.06.2023: You will also include random forests classifiers in this notebook this week. </span>\n",
    "\n",
    "<span style='color:red'> 09.06.2023: You will also include support vector machine classifiers in this notebook this week. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550ddff",
   "metadata": {},
   "source": [
    "We begin by loading the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a437697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # library for organizing data\n",
    "import numpy as np # library for numerial computations\n",
    "from sklearn import linear_model # the linear_model library establishes a straightforward implementation of a linear regression model\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score # this library enables the splitting of a data set into training and test data\n",
    "from sklearn.inspection import DecisionBoundaryDisplay # library to display decision boundaries of classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, RocCurveDisplay, roc_curve, roc_auc_score, log_loss, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt # library for plotting (not interactive)\n",
    "import matminer.datasets as mm # library for data mining materials properties, accesses published datasets\n",
    "from matminer.featurizers.conversions import StrToComposition # converts a string denoting a material composition into the composition\n",
    "from matminer.featurizers.composition.element import ElementFraction # determines the element fraction for a given composition\n",
    "from pymatgen.core import Composition # materials analysis library, module used to analyse the chemical composition of a compound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc820e82",
   "metadata": {},
   "source": [
    "### Predicting metallicity\n",
    "\n",
    "For classification of metallicit based on the material compositon, we will use the dataset based on the publication from [Zhuo et al., 2018](https://pubs.acs.org/doi/pdf/10.1021/acs.jpclett.8b00124)<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) that can be accessed through __matminer__. Have a look at last week's notebook to learn more about the dataset and __matminer__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368751b",
   "metadata": {},
   "source": [
    "In this case, we will start by loading the pickle file which we prepared previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1efda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle('.\\ismetal_df.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1eb46",
   "metadata": {},
   "source": [
    "Assign the dataframe __without__ columns \"formula\" \"is_metal\" and \"composition\" to your feature variable X and the \"is_metal\" column to the observation y. Then split this data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1592db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the variables\n",
    "X=df.drop([\"formula\",\"is_metal\",\"composition\"],axis=1)\n",
    "y=df[\"is_metal\"]\n",
    "\n",
    "# split your data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f068d",
   "metadata": {},
   "source": [
    "Now we will sort the data we created above using different algorithms. To do so, use the train data to fit and the test data to predict the classifiers. Do this by employing the [__logistic regression__](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression), the [__k-nearest neighbors__](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), and the [__decision tree__](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) classification methods. In the case of classification using __k-nearest neighbors__, we will make predictions considering different numbers of nearest neighbors. The same will be done for the __decision tree__ and the maximum depth of the tree. This can be performed within a loop, where you will __append__ the current model and predictions to a list thereof, which will initially be empty. For the loop, consider the range from $\\left[1, 20\\right]$. Read the documentation for these methods if necessary. \n",
    "\n",
    "<span style='color:red'> 02.06.2023: add the [__random forest__](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#) classifier, looping over the number of estimators to include in the ensemble from $\\left[1, 100\\right]$ in steps of 10. </span>\n",
    "\n",
    "<span style='color:red'> 09.06.2023: add the [__support vector machine__](https://scikit-learn.org/stable/modules/svm.html#classification) classifier(s) according to the specification developed in the original paper (try setting the parameters as given in the paper or not). </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "model_log_reg = linear_model.LogisticRegression()\n",
    "model_log_reg=model_log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = model_log_reg.predict(X_test)\n",
    "\n",
    "# k-nearest neighbors (loop over k)\n",
    "model_knn=[]\n",
    "y_pred_knn=[]\n",
    "for k in range(1,21):\n",
    "    model_knn_tmp = KNeighborsClassifier(n_neighbors=k)\n",
    "    model_knn.append(model_knn_tmp.fit(X_train, y_train))\n",
    "    y_pred_knn.append(model_knn_tmp.predict(X_test))\n",
    "\n",
    "# decision tree (loop over max_depth)\n",
    "model_tree=[]\n",
    "y_pred_tree=[]\n",
    "for i in range(1,21):\n",
    "    model_tree_tmp = DecisionTreeClassifier(max_depth=i)\n",
    "    model_tree.append(model_tree_tmp.fit(X_train, y_train))\n",
    "    y_pred_tree.append(model_tree_tmp.predict(X_test))\n",
    "    \n",
    "# random forests (loop over n_estimators in steps of 50, i.e. 1, 50, 100, 150...)\n",
    "model_rf=[]\n",
    "y_pred_rf=[]\n",
    "model_rf_tmp = RandomForestClassifier(n_estimators=1) \n",
    "model_rf.append(model_rf_tmp.fit(X_train, y_train))\n",
    "y_pred_rf.append(model_rf_tmp.predict(X_test))\n",
    "for j in range(10,110,10):\n",
    "    model_rf_tmp = RandomForestClassifier(n_estimators=j) \n",
    "    model_rf.append(model_rf_tmp.fit(X_train, y_train))\n",
    "    y_pred_rf.append(model_rf_tmp.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8af27",
   "metadata": {},
   "source": [
    "### Confusion matrices\n",
    "\n",
    "Now that we have made our predictions with the different classifiers, we are interested in checking the generalization performance of our models. This will be done using the [__confusion matrix__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay). To do this, use the metrics module in the scikit library to create confusion matrices for our train and test data. Build confusion matrices for the three types of classifiers used above. You can also observe what happens when you consider different numbers of neighbors (for the k-nearest neighbors) and different tree depths (for the decision tree). What can you conclude from these different results?\n",
    "\n",
    "<span style='color:red'> 02.06.2023: Add the confusion matrices for the random forests with different numbers of estimators too. How do they behave in comparison to the other classifiers? </span>\n",
    "\n",
    "<span style='color:red'> 09.06.2023: Add the confusion matrices for the SVM. How do they behave in comparison to the other classifiers? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the confusion matrices in one figure as three subplots in two rows, one for training and one for testing data\n",
    "k=5\n",
    "depth=10\n",
    "n_est=100\n",
    "norm_meth=None\n",
    "\n",
    "# display the confusion matrices in one figure as three subplots in two rows, one for training and one for testing data\n",
    "fig,ax=plt.subplots(2,5, figsize=(16,8))\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "cm_display_log_reg = ConfusionMatrixDisplay.from_estimator(model_log_reg,X_train,y_train, normalize=norm_meth, colorbar=False,ax=ax[0][0])\n",
    "ax[0][0].set_title('Logistic regression')\n",
    "cm_display_knn = ConfusionMatrixDisplay.from_estimator(model_knn[k-1],X_train,y_train, normalize=norm_meth, colorbar=False,ax=ax[0][1])\n",
    "ax[0][1].set_title('k-nearest neighbours')\n",
    "cm_display_tree = ConfusionMatrixDisplay.from_estimator(model_tree[depth-1],X_train,y_train, normalize=norm_meth, colorbar=False,ax=ax[0][2])\n",
    "ax[0][2].set_title('Decision tree')\n",
    "cm_display_rf = ConfusionMatrixDisplay.from_estimator(model_rf[int(n_est/10)],X_train,y_train, normalize=norm_meth, colorbar=False,ax=ax[0][3])\n",
    "ax[0][3].set_title('Random forest')\n",
    "\n",
    "cm_display_log_reg = ConfusionMatrixDisplay.from_estimator(model_log_reg,X_test,y_test, normalize=norm_meth, colorbar=False,ax=ax[1][0])\n",
    "cm_display_knn = ConfusionMatrixDisplay.from_estimator(model_knn[k-1],X_test,y_test, normalize=norm_meth, colorbar=False,ax=ax[1][1])\n",
    "cm_display_tree = ConfusionMatrixDisplay.from_estimator(model_tree[depth-1],X_test,y_test, normalize=norm_meth, colorbar=False,ax=ax[1][2])\n",
    "cm_display_rf = ConfusionMatrixDisplay.from_estimator(model_rf[int(n_est/10)],X_test,y_test, normalize=norm_meth, colorbar=False,ax=ax[1][3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd7655",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "Based on the results obtained from the confusion matrices, we will build a panda datraframe to visualize the metrics obtained for our test data predictions. Its table should contain the following columns: 'Classifier', 'True negatives', 'False positives', 'False negatives', 'True positives', 'Positive predictive value', 'Negative predictive value', 'True positive rate', 'True negative rate', 'Log loss error', 'Accuracy', 'Area under the curve'. Do this for all four types of classifiers used. Read the documentation found in the [__sklearn.metrics.confusion_matrix__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) module if needed and search for the other metrics accordingly.\n",
    "\n",
    "To start, you will need to initialize a dataframe with the column names and then use the __.loc__ function to add rows.\n",
    "\n",
    "How do the numerical values compare to the confusion matrices you had displayed?\n",
    "\n",
    "<span style='color:red'> 09.06.2023: Add the values for the SVM. How do they behave in comparison to the other classifiers? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set k and max_depth and n_estimators\n",
    "k=20\n",
    "depth=10\n",
    "n_est=10\n",
    "\n",
    "# initialize the pandas dataframe with the correct column names\n",
    "df_metrics=pd.DataFrame(columns=['Classifier','True negatives', 'False positives', 'False negatives','True positives','Positive predictive value','Negative predictive value','True positive rate','True negative rate', 'Log loss error','Accuracy','Area under the curve'])\n",
    "\n",
    "# compute the metrics for each classifier and add the corresponding rows to the dataframe\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_log_reg).ravel()\n",
    "df_metrics.loc[0]=['Logistic regression',tn,fp,fn,tp,tp/(tp+fp),tn/(fn+tn),tp/(tp+fn),tn/(fp+tn),log_loss(y_test, model_log_reg.predict_proba(X_test)),accuracy_score(y_test, y_pred_log_reg),roc_auc_score(y_test, model_log_reg.predict_proba(X_test)[:,1])]\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn[k-1]).ravel()\n",
    "df_metrics.loc[1]=['k-nearest neighbour',tn,fp,fn,tp,tp/(tp+fp),tn/(fn+tn),tp/(tp+fn),tn/(fp+tn),log_loss(y_test, model_knn[k-1].predict_proba(X_test)),accuracy_score(y_test, y_pred_knn[k-1]),roc_auc_score(y_test, model_knn[k-1].predict_proba(X_test)[:,1])]\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_tree[depth-1]).ravel()\n",
    "df_metrics.loc[2]=['Decision tree',tn,fp,fn,tp,tp/(tp+fp),tn/(fn+tn),tp/(tp+fn),tn/(fp+tn),log_loss(y_test, model_tree[depth-1].predict_proba(X_test)),accuracy_score(y_test, y_pred_tree[depth-1]),roc_auc_score(y_test, model_tree[depth-1].predict_proba(X_test)[:,1])]\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_rf[int(n_est/10)]).ravel()\n",
    "df_metrics.loc[3]=['Random Forest',tn,fp,fn,tp,tp/(tp+fp),tn/(fn+tn),tp/(tp+fn),tn/(fp+tn),log_loss(y_test, model_rf[int(n_est/10)].predict_proba(X_test)),accuracy_score(y_test, y_pred_rf[int(n_est/10)]),roc_auc_score(y_test, model_rf[int(n_est/10)].predict_proba(X_test)[:,1])]\n",
    "\n",
    "# display the data frame\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750c8c7",
   "metadata": {},
   "source": [
    "#### ROC curves\n",
    "\n",
    "Now we will use the ROC curves to analyze the performance of our classifiers for prediction. For this task you will need to calculate the probability estimate for all the binary classes of the model. See documentation for the methods predict_proba(...), roc_curve(...) and RocCurveDisplay(...).\n",
    "\n",
    "1. First use the k-nearest neighbors classifier and plot the ROC curve for models where k = 1, 5, 9, 13, 17. How does the number of nearest neighbors influence the ROC curve? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "for i in range(0,20,4):\n",
    "    y_score_knn=model_knn[i].predict_proba(X)[:,1]\n",
    "    fpr_knn, tpr_knn, _ = roc_curve(y, y_score_knn)\n",
    "    roc_display_knn = RocCurveDisplay(fpr=fpr_knn, tpr=tpr_knn).plot(ax,label='%s nearest neighbours' %(i+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bffa2",
   "metadata": {},
   "source": [
    "2. Do the same thing for the decision tree classifier. How does the max_depth influences the ROC curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6aac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "for i in range(0,20,4):\n",
    "    y_score_tree=model_tree[i].predict_proba(X)[:,1]\n",
    "    fpr_tree, tpr_tree, _ = roc_curve(y, y_score_tree)\n",
    "    roc_display_tree = RocCurveDisplay(fpr=fpr_tree, tpr=tpr_tree).plot(ax,label='Decision tree depth %s' %(i+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451aae2f",
   "metadata": {},
   "source": [
    "3. Similarly, display the ROC curve for the random forest classifier for different n_estimators. How does this variable influence the ROC curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01835c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "for i in range(10,50,10):\n",
    "    y_score_rf=model_rf[int(i/10)].predict_proba(X)[:,1]\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y, y_score_rf)\n",
    "    roc_display_rf = RocCurveDisplay(fpr=fpr_rf, tpr=tpr_rf).plot(ax,label='Random forest n=%s' %(int(i)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558082a3",
   "metadata": {},
   "source": [
    "4. Finally, plot the ROC curves for the logistic regression classifier, k-nearest neighbors classifier (for k = 5), the decision tree classifier (for max_depth = 11) and for the random forest classifier (n_estimators=100).  Of course, you can play with the model parameters or divide between testing and training performance. Which differences do you notice between the different  models?\n",
    "\n",
    "<span style='color:red'> 09.06.2023: Add the ROC curve for the best performing SVM. How does it compare? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17488cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_score_log_reg=model_log_reg.predict_proba(X_test)[:,1]\n",
    "fpr_log_reg, tpr_log_reg, _ = roc_curve(y_test, y_score_log_reg)\n",
    "y_score_knn=model_knn[10].predict_proba(X_test)[:,1]\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_score_knn)\n",
    "y_score_tree=model_tree[10].predict_proba(X_test)[:,1]\n",
    "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_score_tree)\n",
    "y_score_rf=model_rf[10].predict_proba(X_test)[:,1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_score_rf)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "roc_displayog_reg = RocCurveDisplay(fpr=fpr_log_reg, tpr=tpr_log_reg).plot(ax,label='Logistic regression')\n",
    "roc_display_knn = RocCurveDisplay(fpr=fpr_knn, tpr=tpr_knn).plot(ax,label='k-nearest neighbours')\n",
    "roc_display_tree = RocCurveDisplay(fpr=fpr_tree, tpr=tpr_tree).plot(ax,label='Decision tree')\n",
    "roc_display_rf = RocCurveDisplay(fpr=fpr_rf, tpr=tpr_rf).plot(ax,label='Random forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f65cd1",
   "metadata": {},
   "source": [
    "Overall, how do the ROC curves compare to the metrics you had displayed in the dataframe above?\n",
    "\n",
    "Using the following cell, you can display the decision tree that you have trained. Run the code for different __max_depth__ and see how your tree changes. What do you conclude from the tree structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf=model_tree[2]\n",
    "y_names=['no metal','metal']\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(clf,  \n",
    "                   feature_names=df.drop([\"formula\",\"is_metal\",\"composition\"],axis=1).columns, \n",
    "                   class_names=y_names,\n",
    "                   filled=True)\n",
    "#fig.savefig(\"decision_tree_depth%d.png\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39a987",
   "metadata": {},
   "source": [
    "With trees and random forests we can determine the [__feature_importances__](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) in a straightforward manner. The following cell lets you compute the feature importances based on the mean decrease in Gini impurity for the random forests you have trained and plot them in descending order. How does this agree with the tree you plotted in the previous cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48481a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=df.drop([\"formula\",\"is_metal\",\"composition\"],axis=1).columns\n",
    "importances = model_rf[2].feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model_rf[2].estimators_], axis=0)\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "sorted_idx = (-importances).argsort()[:20]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "forest_importances[sorted_idx].plot.bar(yerr=std[sorted_idx], ax=ax)\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05074069",
   "metadata": {},
   "source": [
    "To compute the proximity matrix based on your random forests, use the following cell. The code was taken from [this stackoverflow entry](https://stackoverflow.com/questions/18703136/proximity-matrix-in-sklearn-ensemble-randomforestclassifier). You can vary which random forest model you use and have a look at the resulting visualization of proximities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximityMatrix(model, X, normalize=True):      \n",
    "\n",
    "    terminals = model.apply(X)\n",
    "    nTrees = terminals.shape[1]\n",
    "\n",
    "    a = terminals[:,0]\n",
    "    proxMat = 1*np.equal.outer(a, a)\n",
    "\n",
    "    for i in range(1, nTrees):\n",
    "        a = terminals[:,i]\n",
    "        proxMat += 1*np.equal.outer(a, a)\n",
    "\n",
    "    if normalize:\n",
    "        proxMat = proxMat / nTrees\n",
    "\n",
    "    return proxMat   \n",
    "\n",
    "\n",
    "pM=proximityMatrix(model_rf[2], X, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66832528",
   "metadata": {},
   "source": [
    "The proximity matrix $PM$ can be converted into a dissimilarity matrix $DM=1-PM$ and using multidimensional scaling, it can then be projected onto a 2D-plane, as described in [Gilles Louppe - Understanding Random Forests: From Theory to Practice](https://arxiv.org/abs/1407.7502).\n",
    "To do so, we used the sklearn method [MDS](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html)\n",
    "and adapted the transformation and plotting routine described [here](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py). Execute the following two cells to visualize the proximities for our classes metals and non-metals. \n",
    "\n",
    "What does the result suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "transformer=manifold.MDS(n_components=2, dissimilarity='precomputed', n_init=1, normalized_stress=\"auto\")\n",
    "transformer.dissimilarity_matrix_=(1-pM)\n",
    "\n",
    "projections = transformer.fit_transform((1-pM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ea619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def plot_embedding(XX, title):\n",
    "    _, ax = plt.subplots(figsize=(6,6))\n",
    "    XX = MinMaxScaler().fit_transform(XX)\n",
    "\n",
    "    for ff in [True,False]:\n",
    "        ax.scatter(\n",
    "            *XX[y == ff].T,\n",
    "            label=ff,\n",
    "            s=60,\n",
    "            alpha=0.425,\n",
    "            zorder=2,\n",
    "        )\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plot_embedding(projections,'Random forest proximities metallicity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc04a9e",
   "metadata": {},
   "source": [
    "For comparison, you can run the cell below which will compute the proximity matrix and display the results for the digits data set shown in the lecture. This cell is adapted from [the scikit documentation](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py), which is available under the BSD license (© 2007 - 2023, scikit-learn developers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1871b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "def plot_embeddingd(Xd, title):\n",
    "    _, ax = plt.subplots(figsize=(6,6))\n",
    "    Xd = MinMaxScaler().fit_transform(Xd)\n",
    "\n",
    "    for digit in digits.target_names:\n",
    "        ax.scatter(\n",
    "            *Xd[yd == digit].T,\n",
    "            label=f\"${digit}$\",\n",
    "            s=30,\n",
    "            color=plt.cm.Set3(digit),\n",
    "            zorder=2,\n",
    "        )\n",
    "    ax.legend(loc='lower right', bbox_to_anchor=(1.15, 0.025))\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "digits = load_digits()\n",
    "Xd, yd = digits.data, digits.target\n",
    "n_samples, n_features = Xd.shape\n",
    "rf_model=RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(Xd, yd)\n",
    "\n",
    "pMd=proximityMatrix(rf_model, Xd, normalize=True)\n",
    "\n",
    "transformerd=manifold.MDS(n_components=2, dissimilarity='precomputed', n_init=1, normalized_stress=\"auto\")\n",
    "transformerd.dissimilarity_matrix_=(1-pMd)\n",
    "\n",
    "projectionsd = transformerd.fit_transform((1-pMd))\n",
    "\n",
    "plot_embeddingd(projectionsd,'Random forest proximities digit data set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeae930",
   "metadata": {},
   "source": [
    "For the linear support vector machine, we can display the feature importance by sorting the linear coefficients of each feature by (absolute) value. Have a look at the plot below, which displays the first 20 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d708fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=np.array(df.drop([\"formula\",\"is_metal\",\"composition\"],axis=1).columns)\n",
    "\n",
    "svm_importances = pd.Series(model_svm_lin.coef_[0], index=feature_names)\n",
    "sorted_idx = np.absolute(model_svm_lin.coef_[0]).argsort()[::-1][:20]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "np.absolute(svm_importances[sorted_idx]).plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.set_ylabel(\"Feature importance\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68bbe7",
   "metadata": {},
   "source": [
    "This is not possible for kernels other than the linear one, as the separating hyperplane is in a different dimension than the original features. What you can do instead is to use __permutation importance__, similarly as for the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(model_svm_rbf, X_test, y_test)\n",
    "imp_mean=pd.Series(np.absolute(perm_importance.importances_mean), index=feature_names)\n",
    "imp_std=pd.Series(perm_importance.importances_std, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "sorted_idx = imp_mean.argsort()[::-1][:20]\n",
    "imp_mean[sorted_idx].plot.bar(yerr=imp_std[sorted_idx], ax=ax)\n",
    "ax.set_ylabel(\"Feature importance\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2842c2",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1.[^](#cite_ref-1) Y. Zhuo, A.M. Tehrani, and J. Brgoch, J. Phys. Chem. Lett. 2018, 9, 7, 1668–1673, https://doi.org/10.1021/acs.jpclett.8b00124"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
