{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389557e3",
   "metadata": {},
   "source": [
    "## 26.05.2023 ROC, k-nearest neighbour and decision trees\n",
    "\n",
    "Copyright (C) 2023, B. Zeller-Plumhoff\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.html) for more details.\n",
    "\n",
    "This Jupyter Notebook was created by Berit Zeller-Plumhoff for the course \"Data Science for Material Scientists\" at Kiel University. \n",
    "\n",
    "Within the notebook, you will perform a classification of metallicity using logistic regression, $k$-nearest neighbours and decision trees. Varying the $k$ and depth of the tree, you will assess how these influence your results and compare the performance of the different classifiers using ROC metrics and curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550ddff",
   "metadata": {},
   "source": [
    "We begin by loading the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a437697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # library for organizing data\n",
    "import numpy as np # library for numerial computations\n",
    "from sklearn import linear_model # the linear_model library establishes a straightforward implementation of a linear regression model\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score # this library enables the splitting of a data set into training and test data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, RocCurveDisplay, roc_curve, roc_auc_score, log_loss, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt # library for plotting (not interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc820e82",
   "metadata": {},
   "source": [
    "### Predicting metallicity\n",
    "\n",
    "For classification of metallicit based on the material compositon, we will use the dataset based on the publication from [Zhuo et al., 2018](https://pubs.acs.org/doi/pdf/10.1021/acs.jpclett.8b00124)<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) that can be accessed through __matminer__. Have a look at last week's notebook to learn more about the dataset and __matminer__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368751b",
   "metadata": {},
   "source": [
    "In this case, we will start by loading the pickle file which we prepared previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1efda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle('.\\ismetal_df.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1eb46",
   "metadata": {},
   "source": [
    "Assign the dataframe __without__ columns \"formula\" \"is_metal\" and \"composition\" to your feature variable X and the \"is_metal\" column to the observation y. Then split this data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1592db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the variables\n",
    "\n",
    "\n",
    "\n",
    "# split your data sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f068d",
   "metadata": {},
   "source": [
    "Now we will sort the data we created above using different algorithms. To do so, use the train data to fit and the test data to predict the classifiers. Do this by employing the [__logistic regression__](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression), the [__k-nearest neighbors__](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), and the [__decision tree__](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) classification methods. In the case of classification using __k-nearest neighbors__, we will make predictions considering different numbers of nearest neighbors. The same will be done for the __decision tree__ and the maximum depth of the tree. This can be performed within a loop, where you will __append__ the current model and predictions to a list thereof, which will initially be empty. For the loop, consider the range from $\\left[1, 20\\right]$. Read the documentation for these methods if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "\n",
    "\n",
    "# k-nearest neighbors (loop over k)\n",
    "\n",
    "\n",
    "\n",
    "# decision tree (loop over max_depth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8af27",
   "metadata": {},
   "source": [
    "### Confusion matrices\n",
    "\n",
    "Now that we have made our predictions with the different classifiers, we are interested in checking the generalization performance of our models. This will be done using the [__confusion matrix__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay). To do this, use the metrics module in the scikit library to create confusion matrices for our train and test data. Build confusion matrices for the three types of classifiers used above. You can also observe what happens when you consider different numbers of neighbors (for the k-nearest neighbors) and different tree depths (for the decision tree). What can you conclude from these different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set k and max_depth\n",
    "\n",
    "\n",
    "# display the confusion matrices in one figure as three subplots in two rows, one for training and one for testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd7655",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "Based on the results obtained from the confusion matrices, we will build a panda datraframe to visualize the metrics obtained for our test data predictions. Its table should contain the following columns: 'Classifier', 'True negatives', 'False positives', 'False negatives', 'True positives', 'Positive predictive value', 'Negative predictive value', 'True positive rate', 'True negative rate', 'Log loss error', 'Accuracy', 'Area under the curve'. Do this for all three types of classifiers used. Read the documentation found in the [__sklearn.metrics.confusion_matrix__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) module if needed and search for the other metrics accordingly.\n",
    "\n",
    "To start, you will need to initialize a dataframe with the column names and then use the __.loc__ function to add rows.\n",
    "\n",
    "How do the numerical values compare to the confusion matrices you had displayed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set k and max_depth\n",
    "\n",
    "\n",
    "# initialize the pandas dataframe with the correct column names\n",
    "\n",
    "# compute the metrics for each classifier and add the corresponding rows to the dataframe\n",
    "\n",
    "\n",
    "# display the data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750c8c7",
   "metadata": {},
   "source": [
    "#### ROC curves\n",
    "\n",
    "Now we will use the ROC curves to analyze the performance of our classifiers for prediction. For this task you will need to calculate the probability estimate for all the binary classes of the model. See documentation for the methods predict_proba(...), roc_curve(...) and RocCurveDisplay(...).\n",
    "\n",
    "1. First use the k-nearest neighbors classifier and plot the ROC curve for models where k = 1, 5, 9, 13, 17. How does the number of nearest neighbors influence the ROC curve? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e7176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f0ecba2",
   "metadata": {},
   "source": [
    "2. Do the same thing for the decision tree classifier. How does the max_depth influences the ROC curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6aac72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8feace04",
   "metadata": {},
   "source": [
    "3. Finally, plot the ROC curves for the logistic regression classifier, k-nearest neighbors classifier (for k = 5) and for the decision tree classifier (for max_depth = 11). Which differences do you notice between the different  models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17488cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6f65cd1",
   "metadata": {},
   "source": [
    "Overall, how do the ROC curves compare to the metrics you had displayed in the dataframe above?\n",
    "\n",
    "Using the following cell, you can display the decision tree that you have trained. Run the code for different __max_depth__ and see how your tree changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=model_tree[3]\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "treeinfo=tree.plot_tree(clf,  \n",
    "                   feature_names=df.drop([\"formula\",\"is_metal\",\"composition\"],axis=1).columns, \n",
    "                   class_names=str(df['is_metal']),\n",
    "                   filled=True)\n",
    "#fig.savefig(\"decision_tree_depth%d.png\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ad59d",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1.[^](#cite_ref-1) Y. Zhuo, A.M. Tehrani, and J. Brgoch, J. Phys. Chem. Lett. 2018, 9, 7, 1668–1673, https://doi.org/10.1021/acs.jpclett.8b00124"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
